= Add a Hive database connection
:last_updated: 7/6/2020


:toc: true

You can add a connection to a Hive database using ThoughtSpot DataFlow.

Follow these steps:

{% include content/dataflow/add-database-connection.md %}

. After you select the Hive *Connection type*, the rest of the connection properties appear.
 ** link:{{ site.baseurl }}/data-integrate/dataflow/dataflow-hive-reference.html#dataflow-hive-conn-connection-name[Connection name] + Name your connection.
 ** link:{{ site.baseurl }}/data-integrate/dataflow/dataflow-hive-reference.html#dataflow-hive-conn-connection-type[Connection type] + Choose the Hive connection type.
 ** link:{{ site.baseurl }}/data-integrate/dataflow/dataflow-hive-reference.html#dataflow-hive-conn-hiveserver2-ha-configured[HiveServer2 HA configured] + Specify this option if using HiveServer2 High Availability.
 ** link:{{ site.baseurl }}/data-integrate/dataflow/dataflow-hive-reference.html#dataflow-hive-conn-hiveserver2-zookeeper-namespace[HiveServer2 zookeeper namespace] + Specify zookeeper namespace as hivesever2.
This is the default value.
Only when using Hiveserver2 HA.
 ** link:{{ site.baseurl }}/data-integrate/dataflow/dataflow-hive-reference.html#dataflow-hive-conn-host[Host] + Specify the hostname or the IP address of the Hadoop system Only when _not_ using Hiveserver2 HA.
 ** link:{{ site.baseurl }}/data-integrate/dataflow/dataflow-hive-reference.html#dataflow-hive-conn-port[Port] + Specify the port.
Only when _not_ using Hiveserver2 HA.
 ** link:{{ site.baseurl }}/data-integrate/dataflow/dataflow-hive-reference.html#dataflow-hive-conn-hive-security-authentication[Hive security authentication] + Specifies the type of security protocol to connect to the instance.
Based on the type of security select the authentication type and provide details.
 ** link:{{ site.baseurl }}/data-integrate/dataflow/dataflow-hive-reference.html#dataflow-hive-conn-user[User] + Specify the user to connect to Hive.
This user must have data access privileges.
 ** link:{{ site.baseurl }}/data-integrate/dataflow/dataflow-hive-reference.html#dataflow-hive-conn-password[Password] + Specify the password.
 ** link:{{ site.baseurl }}/data-integrate/dataflow/dataflow-hive-reference.html#dataflow-hive-conn-trust-store[Trust store] + Specify the trust store name for authentication For SSL and Kerberos authentication only.
 ** link:{{ site.baseurl }}/data-integrate/dataflow/dataflow-hive-reference.html#dataflow-hive-conn-trust-store-password[Trust store password] + Specify the password for the trust store For SSL and Kerberos authentication only.
 ** link:{{ site.baseurl }}/data-integrate/dataflow/dataflow-hive-reference.html#dataflow-hive-conn-hive-transport-mode[Hive transport mode] + Applicable only for hive process engine.
This specifies the network protocol used for communicating between hive nodes.
 ** link:{{ site.baseurl }}/data-integrate/dataflow/dataflow-hive-reference.html#dataflow-hive-conn-http-path[HTTP path] + This is specified as an option when http transport mode is selected For HTTP transport mode only.
 ** link:{{ site.baseurl }}/data-integrate/dataflow/dataflow-hive-reference.html#dataflow-hive-conn-hadoop-distribution-[Hadoop distribution] + Provide the Hadoop distribution of the connection
 ** link:{{ site.baseurl }}/data-integrate/dataflow/dataflow-hive-reference.html#dataflow-hive-conn-distribution-version[Distribution version] + Provide the version of the Hadoop distribution
 ** link:{{ site.baseurl }}/data-integrate/dataflow/dataflow-hive-reference.html#dataflow-hive-conn-hadoop-conf-path[Hadoop conf path] + By default, the system picks the Hadoop configuration files from the HDFS.
To override, specify an alternate location.
Applies only when using configuration settings that are different from global Hadoop instance settings.
 ** link:{{ site.baseurl }}/data-integrate/dataflow/dataflow-hive-reference.html#dataflow-hive-conn-dfs-ha-configured[DFS HA configured] + Specify if using High Availability for DFS.
 ** link:{{ site.baseurl }}/data-integrate/dataflow/dataflow-hive-reference.html#dataflow-hive-conn-dfs-name-service[DFS name service] + Specify the logical name of the HDFS nameservice.
 ** link:{{ site.baseurl }}/data-integrate/dataflow/dataflow-hive-reference.html#dataflow-hive-conn-dfs-name-node-ids[DFS name node IDs] + Specify a comma-separated list of NameNode IDs.
System uses this property to determine all NameNodes in the cluster.
XML property name is `dfs.ha.namenodes.dfs.nameservices`.
 ** link:{{ site.baseurl }}/data-integrate/dataflow/dataflow-hive-reference.html#dataflow-hive-conn-rpc-address-for-namenode1[RPC address for namenode1] + Specify the fully-qualified RPC address for each listed NameNode.
Defined as `dfs.namenode.rpc-address.dfs.nameservices.name node ID 1`.
For DFS HA and Hadoop Extract only.
 ** link:{{ site.baseurl }}/data-integrate/dataflow/dataflow-hive-reference.html#dataflow-hive-conn-rpc-address-for-namenode2[RPC address for namenode2] + Specify the fully-qualified RPC address for each listed NameNode.
Define as `dfs.namenode.rpc-address.dfs.nameservices.name node ID 2`.
 ** link:{{ site.baseurl }}/data-integrate/dataflow/dataflow-hive-reference.html#dataflow-hive-conn-dfs-host[DFS host] + Specify the DFS hostname or the IP address.
 ** link:{{ site.baseurl }}/data-integrate/dataflow/dataflow-hive-reference.html#dataflow-hive-conn-dfs-port[DFS port] + Specify the associated DFS port.
 ** link:{{ site.baseurl }}/data-integrate/dataflow/dataflow-hive-reference.html#dataflow-hive-conn-default-dfs-location[Default DFS location] + Specify the location for the default source/target location.
 ** link:{{ site.baseurl }}/data-integrate/dataflow/dataflow-hive-reference.html#dataflow-hive-conn-temp-dfs-location[Temp DFS location] + Specify the location for creating temp directory.
 ** link:{{ site.baseurl }}/data-integrate/dataflow/dataflow-hive-reference.html#dataflow-hive-conn-dfs-security-authentication[DFS security authentication] + Select the type of security being enabled.
 ** link:{{ site.baseurl }}/data-integrate/dataflow/dataflow-hive-reference.html#dataflow-hive-conn-hadoop-rpc-protection[Hadoop RPC protection] + Hadoop cluster administrators control the quality of protection using the configuration parameter `hadoop.rpc.protection`.
 ** link:{{ site.baseurl }}/data-integrate/dataflow/dataflow-hive-reference.html#dataflow-hive-conn-hive-principal[Hive principal] + Principal for authenticating hive services
 ** link:{{ site.baseurl }}/data-integrate/dataflow/dataflow-hive-reference.html#dataflow-hive-conn-user-principal[User principal] + To authenticate via a key-tab you must have supporting key-tab file which is generated by Kerberos Admin and also requires the user principal associated with Key-tab ( Configured while enabling Kerberos)
 ** link:{{ site.baseurl }}/data-integrate/dataflow/dataflow-hive-reference.html#dataflow-hive-conn-user-keytab[User keytab] + To authenticate via a key-tab you must have supporting key-tab file which is generated by Kerberos Admin and also requires the user principal associated with Key-tab ( Configured while enabling Kerberos)
 ** link:{{ site.baseurl }}/data-integrate/dataflow/dataflow-hive-reference.html#dataflow-hive-conn-kdc-host[KDC host] + Specify KDC Host Name where as KDC (Kerberos Key Distribution Center) is a service than runs on a domain controller server role (Configured from Kerbores configuration-/etc/krb5.conf )
 ** link:{{ site.baseurl }}/data-integrate/dataflow/dataflow-hive-reference.html#dataflow-hive-conn-default-realm[Default realm] + A Kerberos realm is the domain over which a Kerberos authentication server has the authority to authenticate a user, host or service (Configured from Kerbores configuration-/etc/krb5.conf )
 ** link:{{ site.baseurl }}/data-integrate/dataflow/dataflow-hive-reference.html#dataflow-hive-conn-queue-name[Queue name] + Specify the queue name followed by a coma separated form in yarn.scheduler.capacity.root.queues.
For Hadoop Extract only.
 ** link:{{ site.baseurl }}/data-integrate/dataflow/dataflow-hive-reference.html#dataflow-hive-conn-yarn-web-ui-port[YARN web UI port] + Yarn Providing web UI for yarn RM and by default 8088 in use For Hadoop Extract only.
 ** link:{{ site.baseurl }}/data-integrate/dataflow/dataflow-hive-reference.html#dataflow-hive-conn-zookeeper-quorum-host[Zookeeper quorum host] + Specify the value of hadoop.registry.zk.quorum from yarn-site.xml Only when _not_ using Hiveserver2 HA.
 ** link:{{ site.baseurl }}/data-integrate/dataflow/dataflow-hive-reference.html#dataflow-hive-conn-yarn-timeline-webapp-host[Yarn timeline webapp host] + Specify the ip adress of yarn timeline service web application
 ** link:{{ site.baseurl }}/data-integrate/dataflow/dataflow-hive-reference.html#dataflow-hive-conn-yarn-timeline-webapp-port[Yarn timeline webapp port] + Specify the port associated with the yarn timeline service web application
 ** link:{{ site.baseurl }}/data-integrate/dataflow/dataflow-hive-reference.html#dataflow-hive-conn-yarn-timeline-webapp-version[Yarn timeline webapp version] + Specify the version associated with the yarn timeline service web application
 ** link:{{ site.baseurl }}/data-integrate/dataflow/dataflow-hive-reference.html#dataflow-hive-conn-jdbc-options[JDBC options] + Specify the options associated with the JDBC URL.

+
See link:{{ site.baseurl }}/data-integrate/dataflow/dataflow-hive-reference.html#connection-properties[Connection properties] for details, defaults, and examples.
. Click *Create connection*.
